## thinking-week22

#### 22.1 机器学习中的监督学习、非监督学习、强化学习有何区别？

答：监督学习是有标签的，有反馈的。非监督学习是没有标签的，没有反馈的。强化学习是没有标签，有反馈的，是从实践中学习。

监督学习又可以分为回归和分类两大类任务。非监督学习最常见的就是聚类。强化学习例如宝宝学走路。

#### 22.2 什么是策略网络，价值网络，有何区别？

答：策略网络是对于给定的输入，通过学习给出一个确定输出的网络：（动作1，状态1），（动作2， 状态2）......

价值网络是通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络。

在围棋中，策略网络的输出，是一个落子的概率分布。价值网络的输出是一个可能获胜的数值，即“价值”，这个价值训练是一种回归(regression)，即调整网络的权重来逼近每一种棋局真实的输赢预测。对于价值网络，当前局面的价值=对终局的估计。

#### 22.3 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的？

答：MCTS是一个搜索算法，它采用的各种方法都是为了有效地减少搜索空间。在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树。

Select：从根节点开始，按一定策略，搜索到叶子节点。

Expansion：对叶子节点扩展一个或多个合法的子节点。

Simluation：对子节点采用随机的方式（这也是为什么称之为蒙特卡洛的原因）模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数。

Backpropagation：根据子节点若干次模拟的得分，更新当前子节点的模拟次数与得分值。同时将模拟次数与得分值回传到其所有祖先节点并更新祖先节点。

#### 22.4 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑？

答：这里的强化学习的奖励可以根据用户最终是否点击了这个视频以及是否完整观看等来设定。神经网络的输入可以是用户的历史观看记录，输出可以是下一个推荐视频的概率p及价值v。再将输出值经过MCTS实现策略改善。最后将结果回传给神经网络。

#### 22.5 在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路。

答：首先可以在虚拟环境中进行模拟。通当前的状态，历史记录等训练一个神经网络，输出下一时刻的应该的操作的概率p及价值v。再将输出值经过MCTS实现策略改善。再将输出值反馈到回传给神经网站中。







